\section{Performance of an Example Use Case}

As an example, we use a simplified infectious disease modeling problem to
demonstrate and benchmark the current state of the HOOVER framework.

Our simple infectious disease model is expressed on a two-dimensional problem
space, which represents some geographic area. Partitions are created as a
regular, two-dimensional grid across the whole problem space.

Each actor in the problem is given 7 attributes:

\begin{enumerate}
    \item A two-component position.
    \item A two-component home location for this vertex.
    \item A two-component current destination location for this vertex.
    \item A single attribute indicating if a vertex is infected or uninfected.
\end{enumerate}

On each timestep, each actor does the following:

\begin{enumerate}
    \item If the current actor is still uninfected, it iterates over all
        vertices with which it shares edges and checks if any are infected. If
        any are infected, the current actor marks itself infected. If the actor
        which infects this actor comes from another PE, the current PE becomes
        coupled with the remote PE.
    \item The current actor then updates its current position based on its home
        location and its destination location. An actor's home location is some
        point in the 2D problem space which it never moves more than a certain
        distance from. A point's destination location is the current point in
        the 2D problem space that an actor is moving towards, which must be near
        its home location.
        \begin{enumerate}
            \item If the actor has not yet reached its destination location, it
                simply updates its current location to continue moving towards
                it.
            \item If the actor has reached its destination location, it
                computes a new destination location within some radius of its
                home and begins moving towards its new destination.
        \end{enumerate}
\end{enumerate}

Benchmarking scalability of irregular problems like those that the HOOVER
framework targets can be difficult, as even small changes in the scale of the
problem or compute resources available can drastically impact the communication
or computational patterns of the application.

Additionally, we would like to strongly emphasize that all performance results shown here
are works-in-progress and that this report will be frequently updated as scaling
improves. Performance bottlenecks and issues continue to be ironed
out of the HOOVER framework, and scalability improves on a weekly basis.
However, this report will serve as a useful document for tracking those
improvements.

Still, we try to use this example problem to illustrate the current scaling
characteristics of the HOOVER framework as several parameters and tunables are
changed.

These experiments are run on a small cluster at Los Alamos National Laboratory
consisting of SGI/HPE C1104-GP2 servers connected with Mellanox ConnectX5.
Each server has 2 sockets, each containing 8 hyperthreaded cores, and 64 GB of
memory. In the experiments below, we run with one PE per core (i.e. 8 PEs per
socket).

As performance continues to improve over time, this report will be updated with
those improvements. Table~\ref{tab:hoover_versions} maps from version numbers used in the text
below to commit hashes in the Github repo. In general, any figures/tables will
include in their caption which version of HOOVER the results are collected from.

\begin{table}
\centering
\begin{tabular}{ | l | l | l | }
\hline
\textbf{Version Number} & \textbf{Git Hash}                         & \textbf{Date} \\\hline
0.1                     & \texttt{385911bb27f74fd74a8f038f95a2447cda372ec4} & Feb 10 2018 \\\hline
0.2                     & \texttt{38591d9adf732c144ced382175348031f8518bad} & Feb 27 2018 \\\hline
0.3                     & \texttt{4f9350fcc2d624ae14cffde6aae08b63a7d3ad16} & Mar 02 2018 \\\hline
0.4                     & \texttt{0129e2a81c2a14959e8966965b25c3fc7cf5c752} & Mar 17 2018 \\\hline
0.5                     & \texttt{09631dfeb925adaccb52f63ff4eaad4ba4fc8243} & Apr 08 2018 \\\hline
0.6                     & \texttt{26deaf397ee98ca785bd381528047ef79790814c} & May 02 2018 \\\hline
0.7                     & \texttt{bc770cc2bdf00b2a9a4d26632d47d5d7d6a8b061} & June 03 2018 \\\hline
\end{tabular}
\caption{Strong scaling of the HOOVER framework on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:hoover_versions}
\end{table}

These experiments are run with OSSS OpenSHMEM over UCX. HOOVER is compiled using
gcc 6.3.0 with -O2 optimization turned on.

\subsection{Strong Scaling}
\label{sec:strong_scaling}

These experiments test the strong scaling of HOOVER while varying two
simulation parameters but keeping the problem space fixed. We vary the number of
timesteps/iterations to run the simulation for, and the infection radius.
Infection radius controls how quickly the infection spreads from one actor to
another, and so an increased infection radius leads to more infected actors
across more PEs (and hence, more rapid PE coupling).
Tables~\ref{tab:strong_scaling1}, \ref{tab:strong_scaling2},
\ref{tab:strong_scaling3}, \ref{tab:strong_scaling4},
and~\ref{tab:strong_scaling5} show the results of these experiments.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | l || X | X | X | X | X | X | }
\hline
\textbf{\# Iters}           & \multicolumn{2}{|X|}{\textbf{10}} & \multicolumn{2}{|X|}{\textbf{100}} & \multicolumn{2}{|X|}{\textbf{200}} \\\hline
\textbf{Infection Radius}   & 4.0          & 10.0         & 4.0           & 10.0          & 4.0           & 10.0 \\\hline
1 node                      & 22,360 & 29,299 & 239,527 & 298,234 & 459,211 & 676,952 \\\hline
4 nodes                     & 1,725  & 3,075  & 16,167  & 32,620  & 32,915  & 66,724 \\\hline
16 nodes                    & 4,389  & 2,916  & 31,517  & 50,068  & 60,293  & 115,186 \\\hline
\end{tabularx}
\caption{Strong scaling of the HOOVER framework v0.1 on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:strong_scaling1}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | l || X | X | X | X | X | X | }
\hline
\textbf{\# Iters}           & \multicolumn{2}{|X|}{\textbf{10}} & \multicolumn{2}{|X|}{\textbf{100}} & \multicolumn{2}{|X|}{\textbf{200}} \\\hline
\textbf{Infection Radius}   & 4.0          & 10.0         & 4.0           & 10.0          & 4.0           & 10.0 \\\hline
1 node                      & 15,169        & 18,321        & 145,107        & 197,610        & 291,928        & 399,285 \\\hline
4 nodes                     & 1,813         & 2,804         & 13,043         & 22,007         & 25,785         & 44,847 \\\hline
16 nodes                    & 1,870         & 1,877         & 7,201          &  12,364        & 9,658          & 27,461 \\\hline
\end{tabularx}
\caption{Strong scaling of the HOOVER framework v0.2 on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:strong_scaling2}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | l || X | X | X | X | X | X | }
\hline
\textbf{\# Iters}           & \multicolumn{2}{|X|}{\textbf{10}} & \multicolumn{2}{|X|}{\textbf{100}} & \multicolumn{2}{|X|}{\textbf{200}} \\\hline
\textbf{Infection Radius}   & 4.0          & 10.0         & 4.0           & 10.0          & 4.0           & 10.0 \\\hline
1 node                      & 12,919 & 18,139 & 142,675 & 193,895 & 303,930 & 401,899 \\\hline
4 nodes                     &  1,817 &  3,043 &  12,321 &  22,917 &  24,266 &  42,755 \\\hline
16 nodes                    &   340 &   657 &   3,204 &  12,166 &   6,996 &  28,947 \\\hline
\end{tabularx}
\caption{Strong scaling of the HOOVER framework v0.3 on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:strong_scaling3}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | l || X | X | X | X | X | X | }
\hline
\textbf{\# Iters}           & \multicolumn{2}{|X|}{\textbf{10}} & \multicolumn{2}{|X|}{\textbf{100}} & \multicolumn{2}{|X|}{\textbf{200}} \\\hline
\textbf{Infection Radius}   & 4.0          & 10.0         & 4.0           & 10.0          & 4.0           & 10.0 \\\hline
4 nodes                     &  1,811       & 2,939        & 14,606       & 24,815       & 30,957       & 45,612 \\\hline
16 nodes                    &   576        &  574         &  4,618       &  5,141       &  9,530       &  9,981 \\\hline
Speedup                     & 3.14$\times$ & 5.12$\times$ & 3.16$\times$ & 4.83$\times$ & 3.25$\times$ & 4.57$\times$ \\\hline
\end{tabularx}
\caption{Strong scaling of the HOOVER framework v0.4 on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:strong_scaling4}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | l || X | X | X | X | X | X | }
\hline
\textbf{\# Iters}           & \multicolumn{2}{|X|}{\textbf{10}} & \multicolumn{2}{|X|}{\textbf{100}} & \multicolumn{2}{|X|}{\textbf{200}} \\\hline
\textbf{Infection Radius}   & 4.0          & 10.0          & 4.0           & 10.0          & 4.0           & 10.0 \\\hline
1 node                      & 10,221       & 26,015        & 108,755       & 264,143       & 211,516       & 548,049 \\\hline
4 nodes                     & 1,356        & 2,196         & 10,161        & 19,682        & 20,720        & 39,490  \\\hline
16 nodes                    & 400          & 330           & 2204          & 2,575         & 4,446         & 5,195 \\\hline
Speedup (1 vs. 4 nodes)     & 7.54$\times$ & 11.85$\times$ & 10.70$\times$ & 13.42$\times$ & 10.21$\times$ & 13.88$\times$ \\\hline
Speedup (4 vs. 16 nodes)    & 3.39$\times$ & 6.65$\times$  & 4.61$\times$  & 7.64$\times$  & 4.67$\times$  & 7.60$\times$ \\\hline
\end{tabularx}
\caption{Strong scaling of the HOOVER framework v0.5 on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:strong_scaling5}
\end{table}


Tables~\ref{tab:strong_scaling1}, \ref{tab:strong_scaling2},
\ref{tab:strong_scaling3}, \ref{tab:strong_scaling4}, and~\ref{tab:strong_scaling5} exhibit a few interesting trends:

\begin{enumerate}
    \item HOOVER does not always exhibit linear strong scaling from 4 to 16
        nodes (16 to 256 PEs), but in most cases does see significant speedup.
    \item In many cases, HOOVER sees super-linear scaling from 1 node to 4
        nodes, likely due to drastic changes in communication patterns as we
        spread the same dataset more thinly across PEs connected by a network.
    \item Increasing the infection radius significantly increases simulation
        time. A likely cause of this is that it increases the number of edges on
        each actor, increasing the amount of time it takes to update each
        actor's attributes when checking for infection.
\end{enumerate}

\subsection{Weak Scaling}
\label{sec:weak_scaling}

These experiments test the weak scaling of HOOVER. We test in multiples of 4
PEs. In each quadrupling of PEs, we increase the problem space size by 2$\times$
on each axis while keeping the number of actors per PE fixed. Hence, with each
quadrupling of PEs the problem space size and number of actors quadruples.
Tables~\ref{tab:weak_scaling1} and~\ref{tab:weak_scaling2} show the results of these experiments.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | l || X | X | X | X | X | X | }
\hline
\textbf{\# Iters}           & \multicolumn{2}{|X|}{\textbf{10}} & \multicolumn{2}{|X|}{\textbf{100}} & \multicolumn{2}{|X|}{\textbf{200}} \\\hline
\textbf{Infection Radius}   & 4.0          & 10.0         & 4.0           & 10.0          & 4.0           & 10.0 \\\hline
1 node                      & 14,957 & 21,729 & 160,171 & 225,808 & 310,550 & 446,786 \\\hline
4 nodes                     & 17,245 & 22,664 & 178,586 & 235,632 & 344,221 & 482,851 \\\hline
16 nodes                    & 22,696 & 28,988 & 232,528 & 297,360 & 469,306 & 600,663 \\\hline
\end{tabularx}
\caption{Weak scaling of the HOOVER framework v0.1 on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:weak_scaling1}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | l || X | X | X | X | X | X | }
\hline
\textbf{\# Iters}           & \multicolumn{2}{|X|}{\textbf{10}} & \multicolumn{2}{|X|}{\textbf{100}} & \multicolumn{2}{|X|}{\textbf{200}} \\\hline
\textbf{Infection Radius}   & 4.0          & 10.0         & 4.0           & 10.0          & 4.0           & 10.0 \\\hline
1 node                      & 9,415  & 12,875 & 96,341  & 135,000 & 198,050 & 289,314 \\\hline
4 nodes                     & 11,820 & 18,629 & 124,190 & 158,152 & 250,276 & 323,039 \\\hline
16 nodes                    & 18,377 & 20,299 & 181,467 & 216,053 & 374,546 & 443,111 \\\hline
\end{tabularx}
\caption{Weak scaling of the HOOVER framework v0.2 on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:weak_scaling2}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | l || X | X | X | X | X | X | }
\hline
\textbf{\# Iters}           & \multicolumn{2}{|X|}{\textbf{10}} & \multicolumn{2}{|X|}{\textbf{100}} & \multicolumn{2}{|X|}{\textbf{200}} \\\hline
\textbf{Infection Radius}   & 4.0          & 10.0         & 4.0           & 10.0          & 4.0           & 10.0 \\\hline
1 node                      &  9,064 & 12,542 &  96,229 & 134,777 & 203,198 & 267,624 \\\hline
4 nodes                     & 15,510 & 16,826 & 122,886 & 158,256 & 259,487 & 337,783 \\\hline
16 nodes                    & 17,130 & 20,889 & 190,952 & 214,298 & 381,488 & 434,551 \\\hline
\end{tabularx}
\caption{Weak scaling of the HOOVER framework v0.3 on OSSS OpenSHMEM running a
    simple infectious disease model with varying \# of iters and infection
    radius.}
\label{tab:weak_scaling3}
\end{table}

Like the strong scaling results, the weak scaling results in
Tables~\ref{tab:weak_scaling1}, \ref{tab:weak_scaling2},
and~\ref{tab:weak_scaling3} show imperfect scaling from 1 to 4 to 16 nodes as
more PEs require more compute time.

\subsection{Larger Scale Tests}

The strong and weak scaling tests described in Sections~\ref{sec:strong_scaling}
and~\ref{sec:weak_scaling} were only run on up to 16 nodes on the small hickok
cluster at LANL.

To better evaluate the scalability of HOOVER, larger strong scaling experiments
were run on the Edison supercomputer at NERSC. Edison has 2x12-core CPUs in each
node with 64 GB of memory. All nodes are connected by the Cray Aries
high performance interconnect.

For these experiments, we continue to use the infectious disease modeling
mini-app with a domain size of 16,000 $\times$ 24,000, and 460,800 actors in
total. This problem size is kept fixed across all runs. Experiments are run with
1 PE per core on 4, 16, 64, and 256 nodes (i.e. 96, 384, 1536, and 6144 PEs).
Table~\ref{tab:large_scale1} shows large scale results with v0.4 of the HOOVER
framework. Table~\ref{tab:large_scale2} shows the same for v0.5.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X || X | X |}
\hline
    \textbf{\# PEs}             & \textbf{Execution Time (ms)} & \textbf{Speedup Relative to Previous} \\\hline
    96                          & 307,611 &               \\\hline
    384                         & 22,259  & 13.82$\times$ \\\hline
    1536                        & 6,568   & 3.39$\times$  \\\hline
    6144                        & 2,418   & 2.72$\times$  \\\hline
\end{tabularx}
\caption{Large scale tests of the HOOVER v0.4 framework on the Edison supercomputer.}
\label{tab:large_scale1}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X || X | X |}
\hline
    \textbf{\# PEs}             & \textbf{Execution Time (ms)} & \textbf{Speedup Relative to Previous} \\\hline
    96                          & 45,326 &              \\\hline
    384                         & 4,762  & 9.52$\times$ \\\hline
    1536                        & 2,230  & 2.14$\times$ \\\hline
    6144                        & 2,070  & 1.08$\times$ \\\hline
\end{tabularx}
\caption{Large scale tests of the HOOVER v0.5 framework on the Edison supercomputer.}
\label{tab:large_scale2}
\end{table}

Even with the performance improvements in HOOVER v0.5, it is clear in
Table~\ref{tab:large_scale2} that we are not linearly scaling out to 6,144 PEs
(though performance improvement does continue). As an experiment, the number of
actors was increased from 460,800 to 4,915,200 and tests rerun.
Table~\ref{tab:large_scale3} plots the results on this larger problem size,
revealing that at larger datasets we do see near linear scaling out to 6,144 PEs.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X || X | X |}
\hline
    \textbf{\# PEs}             & \textbf{Execution Time (ms)} & \textbf{Speedup Relative to Previous} \\\hline
    384                         & 145,161 &               \\\hline
    1,536                       & 13,425  & 10.81$\times$ \\\hline
    6,144                       & 4,237   & 3.17$\times$  \\\hline
    24,576                      & 3,557   & 1.19$\times$ \\\hline
\end{tabularx}
\caption{Larger scale tests of the HOOVER v0.5 framework on the Edison supercomputer.}
\label{tab:large_scale3}
\end{table}

In v0.6 of HOOVER, updates were made to resolve performance issues at small
numbers of PEs (note the poor performance of 384 PEs in
Table~\ref{tab:large_scale3}). Table~\ref{tab:large_scale4} plots the updated
results with these improvements on the same problem size as
Table~\ref{tab:large_scale3}. Table~\ref{tab:large_scale5} doubles the problem
size to look at scaling on an even larger dataset. Note that even for this
larger dataset, at 24,576 PEs the problem is distributed thinly across PEs, with
only 400 actors per PE.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X || X | X |}
\hline
    \textbf{\# PEs}             & \textbf{Execution Time (ms)} & \textbf{Speedup Relative to Previous} \\\hline
    384                         & 32,973 &               \\\hline
    1,536                       & 7,272  & 4.53$\times$ \\\hline
    6,144                       & 4,108  & 1.77$\times$  \\\hline
    24,576                      & 2,979  & 1.38$\times$ \\\hline
\end{tabularx}
\caption{Larger scale tests on 4,915,200 actors with the HOOVER v0.6 framework on the Edison supercomputer.}
\label{tab:large_scale4}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X || X | X |}
\hline
    \textbf{\# PEs}             & \textbf{Execution Time (ms)} & \textbf{Speedup Relative to Previous} \\\hline
    384                         & 157,878 &               \\\hline
    1,536                       & 23,118  & 6.83$\times$ \\\hline
    6,144                       & 9,377   & 2.47$\times$  \\\hline
    24,576                      & 5,865   & 1.60$\times$ \\\hline
\end{tabularx}
\caption{Larger scale tests on 9,830,400 actors with the HOOVER v0.6 framework on the Edison supercomputer.}
\label{tab:large_scale5}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X || X | X |}
\hline
    \textbf{\# PEs}             & \textbf{Execution Time (ms)} & \textbf{Speedup Relative to Previous} \\\hline
    384                         & 56,296  &               \\\hline
    1,536                       & 13,229  & 4.26$\times$ \\\hline
    6,144                       & 6,642   & 1.99$\times$  \\\hline
    24,576                      & Not Run & \\\hline
\end{tabularx}
\caption{Larger scale tests on 9,830,400 actors with the HOOVER v0.7 framework on the Edison supercomputer.}
\label{tab:large_scale5}
\end{table}

\subsection{Memory Scaling}

At large scales, per-PE memory consumption can become problematic as data
structures sized by the number of PEs grow significantly. Some of the
symmetrically allocated data structures in HOOVER do grow with the number of
PEs, so it is important to look at how memory consumption relates to the number
of PEs used. Table~\ref{tab:memory_scaling} shows these numbers, and shows that
per-PE memory usage is highest at small numbers of PEs and large numbers of PEs.
Because these are strong scaling experiments, at small numbers of PEs we have
many more actors per PE and so memory consumption is high. At larger numbers of
PEs, data structures that grow with the number of PEs are significantly larger.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X || X |}
\hline
    \textbf{\# PEs}             & \textbf{Approximate Per-PE Memory Usage (MB)} \\\hline
    96                          & 1,291 \\\hline
    384                         & 340 \\\hline
    1,536                       & 156 \\\hline
    6,144                       & 322 \\\hline
    24,576                      & 1,215 \\\hline
\end{tabularx}
\caption{Memory consumption per PE as a function of the number of PEs in a job for the infectious disease model on HOOVER v0.5.}
\label{tab:memory_scaling}
\end{table}

\subsection{Parameter Design Space Exploration}

The HOOVER runtime itself has many parameters that can be varied. In the
sections that follow, we explore how varying these parameters impacts execution
time for a fixed problem size and number of nodes. We focus on 16 nodes, 100
timesteps, and an infection radius of 10.0.

\subsection{Sparse Vector Buckets}

One tunable parameter is the amount of history to retain per sparse vector. By
default in the tests run above, each sparse vector retains its history for the
past 1,024 timesteps.

Varying the number of timesteps retained offers an interesting performance
tradeoff. While reducing the retained timesteps reduces the size of the sparse
vector data structure and the bytes over the network, it also can reduce the amount
of inter-PE asynchrony that is possible as PEs will begin to throttle themselves
if they detect they are beginning to exceed the number of retained timesteps.

Tables~\ref{tab:timesteps_retained1} and~\ref{tab:timesteps_retained2} clearly shows the performance benefit of
reduced bytes over the wire from a reduced number of retained timesteps. It is
possible that for only 100 timesteps, the PE throttling does not become a major
factor.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X | X | }
\hline
Number of timesteps retained & Execution time (ms) \\\hline
100 & 31,169 \\\hline
50  & 20,639 \\\hline
25  & 28,604 \\\hline
10  & 18,442 \\\hline
2   & 3,964 \\\hline
\end{tabularx}
\caption{Execution time as a function of retained timesteps in each sparse vector on HOOVER v0.1.}
\label{tab:timesteps_retained1}
\end{table}

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X | X | }
\hline
Number of timesteps retained & Execution time (ms) \\\hline
100 & 83,020 \\\hline
50  & 78,255 \\\hline
25  & 102,503 \\\hline
10  & 65,855 \\\hline
2   & 66,070 \\\hline
\end{tabularx}
\caption{Execution time as a function of retained timesteps in each sparse
    vector on HOOVER v0.2 (with a larger dataset than the previous tests, the
    same dataset as was used for weak scaling in the previous section).}
\label{tab:timesteps_retained2}
\end{table}

\subsection{Remote Sparse Vector Cache Size}

As a simple optimization, HOOVER supports caching of remote sparse vectors in a
local PE. This optimization is particularly useful when fetching remote actors
with edges shared with local actors, as many remote actors may have multiple
edges shared with local actors. This cache allows us to fetch a read-only copy
of the remote actor once, and then re-use it.

The remote actor cache is designed as a simple hashmap, with a fixed number of
top-level buckets (keyed on the offset of the remote actor in the remote PE's
list of actors) and a maximum number of entries allowed per bucket. Hence, we
can expect that reducing both the number of top-level buckets and the number of
entries allowed per bucket will both increase cache bucket evictions.

Table~\ref{tab:cache} illustrates that for very small numbers of
top-level buckets and entries per bucket, performance suffers. However, HOOVER
appears to be fairly resilient to varying values for each cache parameter, with
the same performance across many different configurations.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X | X | X | }
\hline
Number of top-level buckets & Max entries per bucket & Execution time (ms) \\\hline
64                          & 64                     & 43,620 \\\hline
64                          & 16                     & 41,276 \\\hline
64                          & 4                      & 41,505 \\\hline
16                          & 64                     & 49,284 \\\hline
16                          & 16                     & 53,973 \\\hline
16                          & 4                      & 47,930 \\\hline
4                           & 64                     & 42,128 \\\hline
4                           & 16                     & 59,965 \\\hline
4                           & 4                      & 81,023 \\\hline
\end{tabularx}
\caption{Execution time as a function of remote cache configuration on HOOVER v0.1.}
\label{tab:cache}
\end{table}

\subsection{Number of Partitions}

The importance of partitions was explained in Section~\ref{sec:edge_updates}. In
this section, we study how varying the number of partitions for the infectious
disease simulator affects performance. By default, in previous experiments we
have used 170$\times$170 = 28,900 partitions formatted as a regular grid on the
two-dimensional problem space. Table~\ref{tab:partitions} shows how performance
changes with partition configuration.

\begin{table}
\centering
\begin{tabularx}{\textwidth}{ | X | X | }
\hline
Partition Configuration & Execution time (ms) \\\hline
50$\times$50            & 13,090 \\\hline
100$\times$100          & 11,047 \\\hline
150$\times$150          & 18,875 \\\hline
200$\times$200          & 14,472 \\\hline
\end{tabularx}
\caption{Execution time as a function of partition configuration for the
    infectious disease modeling application on HOOVER v0.1.}
\label{tab:partitions}
\end{table}
